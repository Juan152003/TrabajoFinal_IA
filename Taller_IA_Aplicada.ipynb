{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Juan David Abaunza**"
      ],
      "metadata": {
        "id": "0ii3U_LQ7cqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taller 3: Redes Neuronales para Reconocimiento de Imágenes**"
      ],
      "metadata": {
        "id": "Yxt31cjq7Mkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 1: Preparación del entorno y datos\n",
        "En este primer paso, se configura el entorno de trabajo y se importa un conjunto de datos de imágenes para su clasificación.**"
      ],
      "metadata": {
        "id": "fSO8IAgA7P-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU5qYmlu7ERd"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Cargamos el conjunto de datos CIFAR-10\n",
        "# Este dataset contiene 60,000 imágenes a color de 32x32 píxeles en 10 categorías\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalizamos los valores de píxeles al rango [0, 1]\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Convertimos las etiquetas a formato one-hot\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Definimos los nombres de las clases para su visualización\n",
        "class_names = ['Avión', 'Automóvil', 'Pájaro', 'Gato', 'Ciervo',\n",
        "               'Perro', 'Rana', 'Caballo', 'Barco', 'Camión']\n",
        "\n",
        "# Función para visualizar imágenes del dataset\n",
        "def visualizar_imagenes(images, labels, class_names):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        # Convertimos la etiqueta one-hot a índice\n",
        "        class_index = np.argmax(labels[i])\n",
        "        plt.title(class_names[class_index])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Llamar a la función para visualizar algunas imágenes\n",
        "visualizar_imagenes(train_images, train_labels, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 2: Construcción de una CNN básica\n",
        "Ahora se construye una red neuronal convolucional simple para clasificar las imágenes.**"
      ],
      "metadata": {
        "id": "KfeAn3rbO8BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_modelo_cnn():\n",
        "    # Creamos un modelo secuencial\n",
        "    modelo = models.Sequential()\n",
        "\n",
        "    # Bloque convolucional 1\n",
        "    modelo.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "    modelo.add(layers.Dropout(0.2))\n",
        "\n",
        "    # Bloque convolucional 2\n",
        "    modelo.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "    modelo.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Bloque convolucional 3\n",
        "    modelo.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "    modelo.add(layers.Dropout(0.4))\n",
        "\n",
        "    # Capas totalmente conectadas\n",
        "    modelo.add(layers.Flatten())\n",
        "    modelo.add(layers.Dense(256, activation='relu'))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Dropout(0.5))\n",
        "    modelo.add(layers.Dense(10, activation='softmax'))  # Capa de salida\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    modelo.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return modelo\n",
        "\n",
        "# Creamos el modelo\n",
        "modelo_cnn = crear_modelo_cnn()\n",
        "\n",
        "# Mostramos un resumen de la arquitectura del modelo\n",
        "modelo_cnn.summary()"
      ],
      "metadata": {
        "id": "FesndrjtMjDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 3: Entrenamiento del modelo\n",
        "En este paso se entrena el modelo con los datos de entrenamiento y se evalúa su rendimiento.**"
      ],
      "metadata": {
        "id": "JeeviQaXPV-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entrenar_modelo(modelo, train_images, train_labels, test_images, test_labels, epochs=10):\n",
        "    # Entrenamos el modelo y guardamos el historial\n",
        "    historial = modelo.fit(\n",
        "        train_images,  # Datos de entrenamiento\n",
        "        train_labels,  # Etiquetas de entrenamiento\n",
        "        epochs=epochs,  # Número de épocas\n",
        "        batch_size=64,  # Tamaño del lote\n",
        "        validation_data=(test_images, test_labels),  # Datos de validación\n",
        "        verbose=1  # Muestra progreso\n",
        "    )\n",
        "\n",
        "    # Evaluamos el modelo con los datos de prueba\n",
        "    resultados = modelo.evaluate(test_images, test_labels, verbose=0)\n",
        "    print(f\"\\nResultados finales:\")\n",
        "    print(f\"Pérdida en datos de prueba: {resultados[0]:.4f}\")\n",
        "    print(f\"Precisión en datos de prueba: {resultados[1]:.4f}\")\n",
        "\n",
        "    return historial\n",
        "\n",
        "# Función para visualizar el historial de entrenamiento\n",
        "def visualizar_historial(historial):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Gráfico de precisión\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(historial.history['accuracy'], label='Precisión entrenamiento')\n",
        "    plt.plot(historial.history['val_accuracy'], label='Precisión validación')\n",
        "    plt.title('Precisión durante el entrenamiento')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Precisión')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Gráfico de pérdida\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(historial.history['loss'], label='Pérdida entrenamiento')\n",
        "    plt.plot(historial.history['val_loss'], label='Pérdida validación')\n",
        "    plt.title('Pérdida durante el entrenamiento')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Pérdida')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Entrenamos el modelo\n",
        "print(\"Iniciando entrenamiento...\")\n",
        "historial = entrenar_modelo(modelo_cnn, train_images, train_labels, test_images, test_labels, epochs=10)\n",
        "\n",
        "# Visualizamos el historial de entrenamiento\n",
        "print(\"\\nVisualizando progreso del entrenamiento...\")\n",
        "visualizar_historial(historial)"
      ],
      "metadata": {
        "id": "ED_CNIazPYg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 4: Predicciones y evaluación visual\n",
        "Ahora se utiliza el modelo entrenado para hacer predicciones y visualizar los resultados.**"
      ],
      "metadata": {
        "id": "a2q7xmUAnUM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_y_visualizar(modelo, images, labels, class_names, num_imagenes=5):\n",
        "    # Seleccionamos algunas imágenes aleatorias\n",
        "    indices = np.random.choice(len(images), num_imagenes, replace=False)\n",
        "\n",
        "    # Predecimos las clases\n",
        "    predicciones = modelo.predict(images[indices])\n",
        "    clases_predichas = np.argmax(predicciones, axis=1)\n",
        "    clases_reales = np.argmax(labels[indices], axis=1)\n",
        "\n",
        "    # Visualizamos las imágenes con predicciones\n",
        "    plt.figure(figsize=(12, 4 * num_imagenes))\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(num_imagenes, 1, i+1)\n",
        "        plt.imshow(images[idx])\n",
        "\n",
        "        # Comprobamos si la predicción es correcta\n",
        "        prediccion_correcta = (clases_predichas[i] == clases_reales[i])\n",
        "        color = 'green' if prediccion_correcta else 'red'\n",
        "\n",
        "        # Mostramos etiqueta real y predicción\n",
        "        plt.title(f\"Real: {class_names[clases_reales[i]]} | Predicción: {class_names[clases_predichas[i]]}\",\n",
        "                 color=color, fontsize=12)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Llamamos a la función para visualizar algunas predicciones\n",
        "predecir_y_visualizar(modelo_cnn, test_images, test_labels, class_names, num_imagenes=5)\n",
        "\n",
        "# Calculamos la matriz de confusión\n",
        "def mostrar_matriz_confusion(modelo, images, labels, class_names):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Predecimos todas las imágenes\n",
        "    predicciones = modelo.predict(images)\n",
        "    clases_predichas = np.argmax(predicciones, axis=1)\n",
        "    clases_reales = np.argmax(labels, axis=1)\n",
        "\n",
        "    # Calculamos la matriz de confusión\n",
        "    matriz_conf = confusion_matrix(clases_reales, clases_predichas)\n",
        "\n",
        "    # Visualizamos la matriz\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(matriz_conf, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Matriz de Confusión', fontsize=16)\n",
        "    plt.xlabel('Predicciones', fontsize=14)\n",
        "    plt.ylabel('Reales', fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Mostramos la matriz de confusión\n",
        "mostrar_matriz_confusion(modelo_cnn, test_images, test_labels, class_names)"
      ],
      "metadata": {
        "id": "qu6bTuZQnZrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 5: Mejoras y experimentación\n",
        "Finalmente, se experimenta con técnicas para mejorar el rendimiento del modelo entrenado.**"
      ],
      "metadata": {
        "id": "hMF4y2kun8U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def crear_modelo_mejorado():\n",
        "    modelo = models.Sequential()\n",
        "\n",
        "    # Bloque 1\n",
        "    modelo.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(1e-4),\n",
        "                           input_shape=(32, 32, 3)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "    modelo.add(layers.Dropout(0.2))\n",
        "\n",
        "    # Bloque 2\n",
        "    modelo.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "    modelo.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Bloque 3\n",
        "    modelo.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "    modelo.add(layers.Dropout(0.4))\n",
        "\n",
        "    # Capas completamente conectadas\n",
        "    modelo.add(layers.Flatten())\n",
        "    modelo.add(layers.Dense(512, activation='relu',\n",
        "                          kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    modelo.add(layers.BatchNormalization())\n",
        "    modelo.add(layers.Dropout(0.5))\n",
        "    modelo.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Optimizador personalizado\n",
        "    optimizer = Adam(learning_rate=0.001, decay=1e-6)\n",
        "\n",
        "    modelo.compile(optimizer=optimizer,\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    return modelo\n",
        "\n",
        "def aplicar_data_augmentation():\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    return datagen\n",
        "\n",
        "def experimentar_hiperparametros():\n",
        "    # Prueba diferentes configuraciones\n",
        "    configs = [\n",
        "        {'filters': [64, 128, 256], 'dropout': [0.2, 0.3, 0.4], 'lr': 0.001},\n",
        "        {'filters': [32, 64, 128], 'dropout': [0.3, 0.4, 0.5], 'lr': 0.0005},\n",
        "        {'filters': [128, 256, 512], 'dropout': [0.1, 0.2, 0.3], 'lr': 0.002}\n",
        "    ]\n",
        "\n",
        "    for config in configs:\n",
        "        print(f\"\\nProbando configuración: {config}\")\n",
        "\n",
        "        # Crear modelo con configuración actual\n",
        "        modelo = models.Sequential()\n",
        "\n",
        "        # Bloque 1\n",
        "        modelo.add(layers.Conv2D(config['filters'][0], (3, 3), activation='relu', padding='same',\n",
        "                               input_shape=(32, 32, 3)))\n",
        "        modelo.add(layers.BatchNormalization())\n",
        "        modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "        modelo.add(layers.Dropout(config['dropout'][0]))\n",
        "\n",
        "        # Bloque 2\n",
        "        modelo.add(layers.Conv2D(config['filters'][1], (3, 3), activation='relu', padding='same'))\n",
        "        modelo.add(layers.BatchNormalization())\n",
        "        modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "        modelo.add(layers.Dropout(config['dropout'][1]))\n",
        "\n",
        "        # Bloque 3\n",
        "        modelo.add(layers.Conv2D(config['filters'][2], (3, 3), activation='relu', padding='same'))\n",
        "        modelo.add(layers.BatchNormalization())\n",
        "        modelo.add(layers.MaxPooling2D((2, 2)))\n",
        "        modelo.add(layers.Dropout(config['dropout'][2]))\n",
        "\n",
        "        # Capas FC\n",
        "        modelo.add(layers.Flatten())\n",
        "        modelo.add(layers.Dense(256, activation='relu'))\n",
        "        modelo.add(layers.Dropout(0.5))\n",
        "        modelo.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "        # Compilar\n",
        "        optimizer = Adam(learning_rate=config['lr'])\n",
        "        modelo.compile(optimizer=optimizer,\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "        # Data augmentation\n",
        "        datagen = aplicar_data_augmentation()\n",
        "\n",
        "        # Entrenamiento con menos épocas para prueba\n",
        "        history = modelo.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                           epochs=10,\n",
        "                           validation_data=(test_images, test_labels),\n",
        "                           verbose=0)\n",
        "\n",
        "        # Mostrar resultados\n",
        "        val_acc = history.history['val_accuracy'][-1]\n",
        "        print(f\"Precisión en validación: {val_acc:.4f}\")\n",
        "\n",
        "# Uso recomendado:\n",
        "modelo_mejorado = crear_modelo_mejorado()\n",
        "modelo_mejorado.summary()\n",
        "\n",
        "# Para data augmentation:\n",
        "datagen = aplicar_data_augmentation()\n",
        "datagen.fit(train_images)\n",
        "\n",
        "# Para experimentar con hiperparámetros:\n",
        "# experimentar_hiperparametros()"
      ],
      "metadata": {
        "id": "Yor7uBJNn_p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taller 4: Integración de Reconocimiento de Imágenes con NLP y Chatbot**"
      ],
      "metadata": {
        "id": "PB-UAVoYpHRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 1: Configuración del entorno y modelos base\n",
        "Primero, se configura un entorno con las herramientas necesarias para trabajar con imágenes y lenguaje natural.**"
      ],
      "metadata": {
        "id": "eAZ5gbbjqHHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Descargamos recursos de NLTK necesarios para NLP\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Cargamos un modelo pre-entrenado para reconocimiento de imágenes\n",
        "def cargar_modelo_imagenes():\n",
        "    # Usamos MobileNetV2 pre-entrenado en ImageNet (ligero y eficiente)\n",
        "    modelo_base = MobileNetV2(weights='imagenet', include_top=True)\n",
        "    return modelo_base\n",
        "\n",
        "# Implementa funciones para preprocesar imágenes\n",
        "def preprocesar_imagen(ruta_imagen=None, imagen_pil=None):\n",
        "    \"\"\"\n",
        "    Preprocesa una imagen para el modelo de reconocimiento.\n",
        "    Acepta una ruta de archivo o una imagen PIL.\n",
        "    \"\"\"\n",
        "    if ruta_imagen is not None:\n",
        "        # Cargar imagen desde ruta\n",
        "        img = image.load_img(ruta_imagen, target_size=(224, 224))\n",
        "    elif imagen_pil is not None:\n",
        "        # Convertir imagen PIL al formato requerido\n",
        "        if imagen_pil.mode != 'RGB':\n",
        "            imagen_pil = imagen_pil.convert('RGB')\n",
        "        img = imagen_pil.resize((224, 224))\n",
        "    else:\n",
        "        raise ValueError(\"Debe proporcionar ruta_imagen o imagen_pil\")\n",
        "\n",
        "    # Convertir a array numpy y agregar dimensión del batch\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocesar según lo que espera MobileNetV2\n",
        "    img_preprocesada = preprocess_input(img_array)\n",
        "\n",
        "    return img_preprocesada\n",
        "\n",
        "# Inicializa el lemmatizador para procesamiento de texto\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Cargamos el modelo de imágenes\n",
        "modelo_imagenes = cargar_modelo_imagenes()\n",
        "print(\"Modelos base cargados correctamente.\")"
      ],
      "metadata": {
        "id": "tCXmbdxupR0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 2: Implementación del reconocimiento de imágenes\n",
        "Ahora se implementa la funcionalidad para analizar y describir imágenes.**"
      ],
      "metadata": {
        "id": "F04LbkZCq1S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "\n",
        "# Cargar el modelo preentrenado (ResNet50)\n",
        "modelo_imagenes = ResNet50(weights='imagenet')\n",
        "\n",
        "def preprocesar_imagen(ruta_imagen=None, imagen_pil=None):\n",
        "    \"\"\"\n",
        "    Preprocesa una imagen para que sea compatible con el modelo ResNet50.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if ruta_imagen:\n",
        "            img = Image.open(ruta_imagen)\n",
        "        elif imagen_pil:\n",
        "            img = imagen_pil\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # Redimensionar a 224x224 (requerido por ResNet50)\n",
        "        img = img.resize((224, 224))\n",
        "\n",
        "        # Convertir a array numpy y normalizar\n",
        "        img_array = np.array(img)\n",
        "        img_array = preprocess_input(img_array)  # Normalización específica de ResNet50\n",
        "        img_array = np.expand_dims(img_array, axis=0)  # Añadir dimensión del batch (1, 224, 224, 3)\n",
        "\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar la imagen: {e}\")\n",
        "        return None\n",
        "\n",
        "def interpretar_contexto_imagen(predicciones):\n",
        "    \"\"\"\n",
        "    Interpreta el contexto emocional o situacional de una imagen\n",
        "    basado en los objetos detectados.\n",
        "    \"\"\"\n",
        "    contextos = []\n",
        "    objetos = [etiqueta.replace('_', ' ') for (_, etiqueta, _) in predicciones]\n",
        "\n",
        "    # Detección de personas\n",
        "    if any(obj in objetos for obj in ['person', 'man', 'woman', 'child', 'baby']):\n",
        "        contextos.append(\"Parece haber personas en la imagen.\")\n",
        "\n",
        "    # Detección de animales\n",
        "    animales = ['dog', 'cat', 'bird', 'horse', 'elephant', 'tiger', 'lion']\n",
        "    if any(obj in objetos for obj in animales):\n",
        "        animales_detectados = [obj for obj in objetos if obj in animales]\n",
        "        contextos.append(f\"Hay presencia de animales como {' y '.join(animales_detectados)}.\")\n",
        "\n",
        "    # Detección de entornos naturales\n",
        "    naturaleza = ['tree', 'mountain', 'beach', 'ocean', 'forest', 'sky']\n",
        "    if any(obj in objetos for obj in naturaleza):\n",
        "        contextos.append(\"La imagen tiene elementos naturales.\")\n",
        "\n",
        "    # Detección de actividades\n",
        "    if 'sports' in objetos or any(obj in objetos for obj in ['ball', 'racket', 'bicycle']):\n",
        "        contextos.append(\"Parece mostrar alguna actividad deportiva.\")\n",
        "    elif 'food' in objetos or any(obj in objetos for obj in ['pizza', 'burger', 'sushi']):\n",
        "        contextos.append(\"La imagen está relacionada con comida.\")\n",
        "\n",
        "    # Determinar el contexto predominante\n",
        "    if not contextos:\n",
        "        return \"\"\n",
        "    elif len(contextos) == 1:\n",
        "        return contextos[0]\n",
        "    else:\n",
        "        return \" \".join([\"Además,\"] + contextos[1:])\n",
        "\n",
        "def analizar_imagen(ruta_imagen=None, imagen_pil=None):\n",
        "    \"\"\"\n",
        "    Analiza una imagen y devuelve una descripción de lo que contiene.\n",
        "    Muestra la imagen antes de realizar el análisis.\n",
        "    \"\"\"\n",
        "    # Cargar y mostrar la imagen\n",
        "    if ruta_imagen:\n",
        "        img_pil = Image.open(ruta_imagen)\n",
        "    elif imagen_pil:\n",
        "        img_pil = imagen_pil\n",
        "    else:\n",
        "        return \"No se proporcionó ninguna imagen.\"\n",
        "\n",
        "    # Mostrar la imagen\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(img_pil)\n",
        "    plt.axis('off')  # Ocultar ejes\n",
        "    plt.title(\"Imagen a analizar\")\n",
        "    plt.show()\n",
        "\n",
        "    # Preprocesar la imagen\n",
        "    img_array = preprocesar_imagen(ruta_imagen, img_pil)\n",
        "\n",
        "    if img_array is None:\n",
        "        return \"No se pudo procesar la imagen.\"\n",
        "\n",
        "    # Realizar la predicción\n",
        "    predicciones = modelo_imagenes.predict(img_array)\n",
        "    etiquetas = decode_predictions(predicciones, top=3)[0]  # Top 3 predicciones\n",
        "\n",
        "    # Formatear los resultados\n",
        "    descripcion = \"En esta imagen puedo ver: \"\n",
        "    objetos_detectados = []\n",
        "\n",
        "    for i, (id, etiqueta, probabilidad) in enumerate(etiquetas):\n",
        "        etiqueta_formateada = etiqueta.replace('_', ' ')\n",
        "        if i == len(etiquetas) - 1 and len(etiquetas) > 1:\n",
        "            objetos_detectados.append(f\"y {etiqueta_formateada}\")\n",
        "        else:\n",
        "            objetos_detectados.append(etiqueta_formateada)\n",
        "\n",
        "    descripcion += ', '.join(objetos_detectados) + \".\"\n",
        "\n",
        "    # Añadir interpretación contextual\n",
        "    contexto = interpretar_contexto_imagen(etiquetas)\n",
        "    if contexto:\n",
        "        descripcion += \" \" + contexto\n",
        "\n",
        "    return descripcion\n",
        "\n",
        "# --- Ejemplo de uso ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Cambia esta ruta por la de tu imagen (se debe subir a archivos, la carpeta que se ve a la izquierda en .jpg y que sea clara)\n",
        "    ruta_imagen_prueba = \"Pajaro.jpg\"  # Ejemplo: \"Perro.jpg\", \"Gato.jpg\", etc.\n",
        "\n",
        "    try:\n",
        "        resultado = analizar_imagen(ruta_imagen_prueba)\n",
        "        print(resultado)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al analizar la imagen: {e}\")"
      ],
      "metadata": {
        "id": "fiwabp-evy3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 3: Desarrollo del motor NLP para el chatbot\n",
        "En este paso, se implementa procesamiento de lenguaje natural para el chatbot.**"
      ],
      "metadata": {
        "id": "qxk4q7Rryia3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Cargar modelo de lenguaje para lematización (español)\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Definimos patrones de conversación básicos relacionados con imágenes y consultas\n",
        "patrones_conversacion = {\n",
        "    \"saludos\": [\n",
        "        r\"hola\",\n",
        "        r\"buenos\\s*d[ií]as\",\n",
        "        r\"buenas\\s*tardes\",\n",
        "        r\"buenas\\s*noches\",\n",
        "        r\"hey\",\n",
        "        r\"saludos\"\n",
        "    ],\n",
        "    \"preguntas_imagen\": [\n",
        "        r\"qué\\s*ves\\s*en\\s*(la|esta)\\s*imagen\",\n",
        "        r\"describe\\s*(la|esta)\\s*imagen\",\n",
        "        r\"qué\\s*hay\\s*en\\s*(la|esta)\\s*imagen\",\n",
        "        r\"qué\\s*puedes\\s*ver\",\n",
        "        r\"analiza\\s*(la|esta)\\s*imagen\"\n",
        "    ],\n",
        "    \"consulta_psicologica\": [\n",
        "        r\"me\\s*siento\\s*(triste|feliz|ansioso|preocupado|estresado)\",\n",
        "        r\"tengo\\s*problemas\\s*con\",\n",
        "        r\"no\\s*puedo\\s*(dormir|concentrarme|estudiar)\",\n",
        "        r\"necesito\\s*ayuda\\s*con\\s*mis\\s*emociones\",\n",
        "        r\"cómo\\s*puedo\\s*manejar\\s*(el\\s*estrés|la\\s*ansiedad|la\\s*depresión)\"\n",
        "    ],\n",
        "    \"consulta_academica\": [\n",
        "        r\"no\\s*entiendo\\s*(este\\s*tema|esta\\s*materia)\",\n",
        "        r\"cómo\\s*puedo\\s*estudiar\\s*mejor\",\n",
        "        r\"tengo\\s*dificultades\\s*con\",\n",
        "        r\"necesito\\s*ayuda\\s*con\\s*mis\\s*estudios\",\n",
        "        r\"cómo\\s*mejorar\\s*(mi\\s*concentración|mi\\s*memoria|mis\\s*notas)\"\n",
        "    ],\n",
        "    \"despedida\": [\n",
        "        r\"adiós\",\n",
        "        r\"hasta\\s*luego\",\n",
        "        r\"nos\\s*vemos\",\n",
        "        r\"chao\",\n",
        "        r\"bye\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Definimos respuestas para cada categoría\n",
        "respuestas = {\n",
        "    \"saludos\": [\n",
        "        \"¡Hola! ¿En qué puedo ayudarte hoy?\",\n",
        "        \"¡Saludos! Soy un asistente virtual. Puedo analizar imágenes y conversar contigo.\",\n",
        "        \"¡Buen día! Estoy aquí para ayudarte con tus consultas e imágenes.\"\n",
        "    ],\n",
        "    \"preguntas_imagen_sin_contexto\": [\n",
        "        \"Para analizar una imagen, necesito que me la proporciones primero.\",\n",
        "        \"No tengo ninguna imagen para analizar. ¿Podrías compartir una?\",\n",
        "        \"Necesito ver una imagen antes de poder describirla.\"\n",
        "    ],\n",
        "    \"consulta_psicologica\": [\n",
        "        \"Entiendo cómo te sientes. ¿Podrías contarme más sobre eso?\",\n",
        "        \"Es normal tener esos sentimientos. ¿Qué crees que los está causando?\",\n",
        "        \"Gracias por compartir eso conmigo. Te escucho. ¿Desde cuándo te sientes así?\"\n",
        "    ],\n",
        "    \"consulta_academica\": [\n",
        "        \"Aprender puede ser desafiante. ¿Con qué tema específico estás teniendo dificultades?\",\n",
        "        \"Cada persona tiene su propio estilo de aprendizaje. ¿Has identificado qué métodos funcionan mejor para ti?\",\n",
        "        \"El éxito académico requiere estrategia. ¿Has probado crear un horario de estudio?\"\n",
        "    ],\n",
        "    \"despedida\": [\n",
        "        \"¡Hasta pronto! Fue un placer ayudarte.\",\n",
        "        \"¡Adiós! Si necesitas más ayuda, aquí estaré.\",\n",
        "        \"¡Que tengas un excelente día! Regresa cuando necesites apoyo.\"\n",
        "    ],\n",
        "    \"default\": [\n",
        "        \"Interesante. Cuéntame más.\",\n",
        "        \"No estoy seguro de entender. ¿Podrías explicarlo de otra manera?\",\n",
        "        \"Estoy aquí para ayudarte. ¿Puedes ser más específico?\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def procesar_texto(texto):\n",
        "    \"\"\"\n",
        "    Procesa el texto de entrada para normalización:\n",
        "    - Convierte a minúsculas\n",
        "    - Elimina caracteres especiales\n",
        "    - Tokeniza y lematiza\n",
        "    - Elimina stopwords\n",
        "    \"\"\"\n",
        "    # Convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Eliminar caracteres especiales (excepto letras, números y espacios)\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "    # Tokenizar y lematizar\n",
        "    doc = nlp(texto)\n",
        "    tokens_procesados = [\n",
        "        token.lemma_ for token in doc\n",
        "        if not token.is_stop and not token.is_punct and token.text.strip()\n",
        "    ]\n",
        "\n",
        "    return tokens_procesados\n",
        "\n",
        "def identificar_intencion(texto):\n",
        "    \"\"\"\n",
        "    Identifica la intención del usuario basado en patrones de expresión regular\n",
        "    Devuelve la categoría de intención detectada\n",
        "    \"\"\"\n",
        "    texto = texto.lower().strip()\n",
        "\n",
        "    # Verificar cada categoría y sus patrones\n",
        "    for categoria, patrones in patrones_conversacion.items():\n",
        "        for patron in patrones:\n",
        "            if re.search(patron, texto):\n",
        "                return categoria\n",
        "\n",
        "    return \"default\"\n",
        "\n",
        "def obtener_respuesta(intencion):\n",
        "    \"\"\"\n",
        "    Devuelve una respuesta aleatoria para la intención detectada\n",
        "    \"\"\"\n",
        "    from random import choice\n",
        "    return choice(respuestas.get(intencion, respuestas[\"default\"]))\n",
        "\n",
        "def chatbot_interactivo():\n",
        "    \"\"\"\n",
        "    Función principal para interactuar con el chatbot\n",
        "    \"\"\"\n",
        "    print(\"¡Hola! Soy tu asistente virtual. Puedo:\")\n",
        "    print(\"- Analizar imágenes (describe lo que veo)\")\n",
        "    print(\"- Escucharte si necesitas apoyo psicológico\")\n",
        "    print(\"- Ayudarte con consultas académicas\")\n",
        "    print(\"Escribe 'salir' para terminar la conversación.\\n\")\n",
        "\n",
        "    while True:\n",
        "        entrada = input(\"Tú: \").strip()\n",
        "\n",
        "        if entrada.lower() in ('salir', 'exit', 'quit'):\n",
        "            print(obtener_respuesta(\"despedida\"))\n",
        "            break\n",
        "\n",
        "        # Procesar el texto\n",
        "        tokens = procesar_texto(entrada)\n",
        "        print(f\"(DEBUG) Tokens procesados: {tokens}\")\n",
        "\n",
        "        # Identificar intención\n",
        "        intencion = identificar_intencion(entrada)\n",
        "        print(f\"(DEBUG) Intención detectada: {intencion}\")\n",
        "\n",
        "        # Obtener y mostrar respuesta\n",
        "        respuesta = obtener_respuesta(intencion)\n",
        "        print(f\"Asistente: {respuesta}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejemplo de uso del procesamiento de texto\n",
        "    test_text = \"Hola, no puedo concentrarme en mis estudios. ¿Qué me recomiendas?\"\n",
        "    print(\"\\nEjemplo de procesamiento:\")\n",
        "    print(f\"Texto original: {test_text}\")\n",
        "    print(f\"Tokens procesados: {procesar_texto(test_text)}\")\n",
        "    print(f\"Intención detectada: {identificar_intencion(test_text)}\")\n",
        "\n",
        "    # Iniciar chatbot interactivo\n",
        "    print(\"\\nModo interactivo:\")\n",
        "    chatbot_interactivo()"
      ],
      "metadata": {
        "id": "-h5Hf-3nypyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 4: Integración de los componentes y creación de la interfaz\n",
        "Ahora se integra el reconocimiento de imágenes con el procesamiento de lenguaje natural.**"
      ],
      "metadata": {
        "id": "tpQ__A1H0ab2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Funciones auxiliares ficticias (deberías implementarlas adecuadamente)\n",
        "def modelo_imagenes():\n",
        "    \"\"\"Simulación de un modelo de análisis de imágenes\"\"\"\n",
        "    return None\n",
        "\n",
        "def analizar_imagen(ruta_imagen=None, imagen_pil=None):\n",
        "    \"\"\"Función simulada para analizar imágenes\"\"\"\n",
        "    if ruta_imagen:\n",
        "        try:\n",
        "            img = Image.open(ruta_imagen)\n",
        "            return {\n",
        "                \"estado\": \"éxito\",\n",
        "                \"descripcion\": f\"Imagen analizada: {os.path.basename(ruta_imagen)}\",\n",
        "                \"detalles\": {\n",
        "                    \"tamaño\": img.size,\n",
        "                    \"formato\": img.format,\n",
        "                    \"modo\": img.mode\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"estado\": \"error\", \"mensaje\": str(e)}\n",
        "    elif imagen_pil:\n",
        "        return {\n",
        "            \"estado\": \"éxito\",\n",
        "            \"descripcion\": \"Imagen PIL analizada\",\n",
        "            \"detalles\": {\n",
        "                \"tamaño\": imagen_pil.size,\n",
        "                \"modo\": imagen_pil.mode\n",
        "            }\n",
        "        }\n",
        "    return {\"estado\": \"error\", \"mensaje\": \"No se proporcionó imagen\"}\n",
        "\n",
        "def identificar_intencion(texto):\n",
        "    \"\"\"Identifica la intención del usuario de forma básica\"\"\"\n",
        "    texto = texto.lower()\n",
        "    if any(palabra in texto for palabra in [\"hola\", \"hi\", \"buenos días\"]):\n",
        "        return \"saludo\"\n",
        "    elif any(palabra in texto for palabra in [\"adiós\", \"chao\", \"hasta luego\"]):\n",
        "        return \"despedida\"\n",
        "    elif any(palabra in texto for palabra in [\"qué\", \"cómo\", \"dime\", \"explica\"]):\n",
        "        return \"pregunta\"\n",
        "    elif \"imagen\" in texto or \"foto\" in texto:\n",
        "        return \"consulta_imagen\"\n",
        "    return \"otro\"\n",
        "\n",
        "class AsistenteVirtual:\n",
        "    def __init__(self):\n",
        "        self.modelo_imagenes = modelo_imagenes()\n",
        "        self.imagen_actual = None\n",
        "        self.analisis_imagen_actual = None\n",
        "        self.contexto_conversacion = []\n",
        "\n",
        "    def procesar_imagen(self, ruta_imagen=None, imagen_pil=None):\n",
        "        \"\"\"\n",
        "        Procesa una nueva imagen y actualiza el contexto\n",
        "        \"\"\"\n",
        "        self.analisis_imagen_actual = analizar_imagen(ruta_imagen, imagen_pil)\n",
        "\n",
        "        if ruta_imagen:\n",
        "            try:\n",
        "                self.imagen_actual = Image.open(ruta_imagen)\n",
        "            except:\n",
        "                self.imagen_actual = None\n",
        "        else:\n",
        "            self.imagen_actual = imagen_pil\n",
        "\n",
        "        return self.analisis_imagen_actual\n",
        "\n",
        "    def responder(self, texto_usuario):\n",
        "        \"\"\"\n",
        "        Genera una respuesta basada en el input del usuario y el contexto actual\n",
        "        \"\"\"\n",
        "        categoria = identificar_intencion(texto_usuario)\n",
        "\n",
        "        if categoria == \"saludo\":\n",
        "            respuesta = \"¡Hola! ¿En qué puedo ayudarte hoy?\"\n",
        "        elif categoria == \"despedida\":\n",
        "            respuesta = \"¡Hasta luego! Fue un placer ayudarte.\"\n",
        "        elif categoria == \"pregunta\":\n",
        "            if self.analisis_imagen_actual:\n",
        "                respuesta = f\"Actualmente estoy analizando una imagen. {self.analisis_imagen_actual['descripcion']}. ¿Quieres saber algo específico sobre ella?\"\n",
        "            else:\n",
        "                respuesta = \"Puedo ayudarte a analizar imágenes. ¿Quieres que analice alguna?\"\n",
        "        elif categoria == \"consulta_imagen\":\n",
        "            if self.analisis_imagen_actual:\n",
        "                detalles = self.analisis_imagen_actual.get('detalles', {})\n",
        "                respuesta = (f\"Actualmente tengo una imagen cargada. Detalles:\\n\"\n",
        "                           f\"Tamaño: {detalles.get('tamaño', 'N/A')}\\n\"\n",
        "                           f\"Formato: {detalles.get('formato', 'N/A')}\\n\"\n",
        "                           f\"Modo: {detalles.get('modo', 'N/A')}\")\n",
        "            else:\n",
        "                respuesta = \"No hay ninguna imagen cargada actualmente. Puedes proporcionarme una con el comando 'ver imagen: [ruta]'\"\n",
        "        else:\n",
        "            respuesta = \"No estoy seguro de cómo responder a eso. ¿Puedes reformular tu pregunta?\"\n",
        "\n",
        "        # Actualiza el contexto de la conversación\n",
        "        self.contexto_conversacion.append({\"usuario\": texto_usuario, \"respuesta\": respuesta})\n",
        "\n",
        "        return respuesta\n",
        "\n",
        "def interfaz_cli():\n",
        "    \"\"\"\n",
        "    Interfaz simple de línea de comandos para interactuar con el asistente\n",
        "    \"\"\"\n",
        "    asistente = AsistenteVirtual()\n",
        "    print(\"=== Asistente Virtual ===\")\n",
        "    print(\"Puedo analizar imágenes y conversar contigo.\")\n",
        "    print(\"Para analizar una imagen, escribe 'ver imagen: [ruta_a_la_imagen]'\")\n",
        "    print(\"Para salir, escribe 'salir'\")\n",
        "\n",
        "    while True:\n",
        "        entrada = input(\"\\nTú: \").strip()\n",
        "\n",
        "        if entrada.lower() == \"salir\":\n",
        "            print(\"Asistente: ¡Hasta pronto!\")\n",
        "            break\n",
        "\n",
        "        if entrada.lower().startswith(\"ver imagen:\"):\n",
        "            ruta = entrada[11:].strip()\n",
        "            if not ruta:\n",
        "                print(\"Asistente: Por favor, proporciona una ruta válida después de 'ver imagen:'\")\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(ruta):\n",
        "                print(\"Asistente: No puedo encontrar ese archivo. ¿Estás seguro de que la ruta es correcta?\")\n",
        "                continue\n",
        "\n",
        "            resultado = asistente.procesar_imagen(ruta_imagen=ruta)\n",
        "            if resultado.get('estado') == 'éxito':\n",
        "                print(f\"Asistente: He analizado la imagen: {os.path.basename(ruta)}\")\n",
        "            else:\n",
        "                print(f\"Asistente: Hubo un problema al analizar la imagen: {resultado.get('mensaje', 'Error desconocido')}\")\n",
        "        else:\n",
        "            respuesta = asistente.responder(entrada)\n",
        "            print(f\"Asistente: {respuesta}\")\n",
        "\n",
        "def interfaz_grafica():\n",
        "    \"\"\"\n",
        "    Interfaz gráfica simple para el asistente virtual usando Tkinter\n",
        "    \"\"\"\n",
        "    import tkinter as tk\n",
        "    from tkinter import filedialog, scrolledtext\n",
        "\n",
        "    class Aplicacion(tk.Tk):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.title(\"Asistente Virtual\")\n",
        "            self.geometry(\"600x500\")\n",
        "            self.asistente = AsistenteVirtual()\n",
        "\n",
        "            self.crear_widgets()\n",
        "\n",
        "        def crear_widgets(self):\n",
        "            # Área de conversación\n",
        "            self.conversacion = scrolledtext.ScrolledText(self, wrap=tk.WORD, state='disabled')\n",
        "            self.conversacion.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
        "\n",
        "            # Entrada de usuario\n",
        "            frame_entrada = tk.Frame(self)\n",
        "            frame_entrada.pack(padx=10, pady=5, fill=tk.X)\n",
        "\n",
        "            self.entrada = tk.Entry(frame_entrada)\n",
        "            self.entrada.pack(side=tk.LEFT, fill=tk.X, expand=True)\n",
        "            self.entrada.bind(\"<Return>\", self.enviar_mensaje)\n",
        "\n",
        "            tk.Button(frame_entrada, text=\"Enviar\", command=self.enviar_mensaje).pack(side=tk.LEFT, padx=5)\n",
        "            tk.Button(frame_entrada, text=\"Cargar Imagen\", command=self.cargar_imagen).pack(side=tk.LEFT, padx=5)\n",
        "\n",
        "            # Mensaje inicial\n",
        "            self.mostrar_mensaje(\"Asistente\", \"¡Hola! Soy tu asistente virtual. Puedo analizar imágenes y conversar contigo.\")\n",
        "\n",
        "        def cargar_imagen(self):\n",
        "            ruta = filedialog.askopenfilename(\n",
        "                title=\"Seleccionar imagen\",\n",
        "                filetypes=[(\"Imágenes\", \"*.jpg *.jpeg *.png *.bmp\"), (\"Todos los archivos\", \"*.*\")]\n",
        "            )\n",
        "\n",
        "            if ruta:\n",
        "                resultado = self.asistente.procesar_imagen(ruta_imagen=ruta)\n",
        "                if resultado.get('estado') == 'éxito':\n",
        "                    self.mostrar_mensaje(\"Asistente\", f\"He analizado la imagen: {os.path.basename(ruta)}\")\n",
        "                else:\n",
        "                    self.mostrar_mensaje(\"Asistente\", f\"Error al analizar la imagen: {resultado.get('mensaje', 'Error desconocido')}\")\n",
        "\n",
        "        def enviar_mensaje(self, event=None):\n",
        "            mensaje = self.entrada.get().strip()\n",
        "            if not mensaje:\n",
        "                return\n",
        "\n",
        "            self.mostrar_mensaje(\"Tú\", mensaje)\n",
        "            self.entrada.delete(0, tk.END)\n",
        "\n",
        "            respuesta = self.asistente.responder(mensaje)\n",
        "            self.mostrar_mensaje(\"Asistente\", respuesta)\n",
        "\n",
        "        def mostrar_mensaje(self, remitente, mensaje):\n",
        "            self.conversacion.configure(state='normal')\n",
        "            self.conversacion.insert(tk.END, f\"{remitente}: {mensaje}\\n\\n\")\n",
        "            self.conversacion.configure(state='disabled')\n",
        "            self.conversacion.see(tk.END)\n",
        "\n",
        "    app = Aplicacion()\n",
        "    app.mainloop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Selecciona el modo de interfaz:\")\n",
        "    print(\"1. Línea de comandos\")\n",
        "    print(\"2. Interfaz gráfica\")\n",
        "    opcion = input(\"Opción (1/2): \").strip()\n",
        "\n",
        "    if opcion == \"1\":\n",
        "        interfaz_cli()\n",
        "    elif opcion == \"2\":\n",
        "        interfaz_grafica()\n",
        "    else:\n",
        "        print(\"Opción no válida. Ejecutando interfaz de línea de comandos por defecto.\")\n",
        "        interfaz_cli()"
      ],
      "metadata": {
        "id": "PxMiNYopC3P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 5: Implementación de funcionalidades avanzadas y casos de uso específicos\n",
        "Finalmente, se implementan las funciones avanzadas y se adapta el asistente para casos de uso específicos.**"
      ],
      "metadata": {
        "id": "Xqij5BbeDywt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementación de funcionalidades específicas para educación\n",
        "def generador_ejercicios(tema, dificultad):\n",
        "    \"\"\"\n",
        "    Genera ejercicios educativos basados en un tema y nivel de dificultad\n",
        "    \"\"\"\n",
        "    ejercicios = {\n",
        "        \"matematicas\": {\n",
        "            \"facil\": [\"2 + 2 = ?\", \"5 × 3 = ?\", \"10 - 4 = ?\"],\n",
        "            \"medio\": [\"3x + 5 = 20, x = ?\", \"Área de un cuadrado con lado 5\", \"√36 = ?\"],\n",
        "            \"dificil\": [\"Derivada de x² + 3x\", \"Integral de 2x dx\", \"Límite de (x²-4)/(x-2) cuando x→2\"]\n",
        "        },\n",
        "        \"vocabulario\": {\n",
        "            \"facil\": [\"Sinónimo de 'feliz'\", \"Antónimo de 'alto'\", \"Definir 'casa'\"],\n",
        "            \"medio\": [\"Usar 'efímero' en una oración\", \"Etimología de 'palabra'\", \"3 palabras que riman con 'sol'\"],\n",
        "            \"dificil\": [\"Diferencia entre 'haber' y 'a ver'\", \"5 palabras cultas para 'comida'\", \"Definir 'paronomasia'\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if tema in ejercicios and dificultad in ejercicios[tema]:\n",
        "        return ejercicios[tema][dificultad]\n",
        "    else:\n",
        "        return [\"Lo siento, no tengo ejercicios para ese tema/dificultad.\"]\n",
        "\n",
        "# Implementación de funciones para apoyo psicológico básico\n",
        "def analisis_sentimiento(texto):\n",
        "    \"\"\"\n",
        "    Realiza un análisis simple de sentimiento en el texto del usuario\n",
        "    \"\"\"\n",
        "    palabras_positivas = {\"feliz\", \"contento\", \"genial\", \"maravilloso\", \"bien\", \"alegre\"}\n",
        "    palabras_negativas = {\"triste\", \"mal\", \"deprimido\", \"ansioso\", \"preocupado\", \"enfadado\"}\n",
        "\n",
        "    texto = texto.lower()\n",
        "    palabras = texto.split()\n",
        "\n",
        "    positivas = sum(1 for palabra in palabras if palabra in palabras_positivas)\n",
        "    negativas = sum(1 for palabra in palabras if palabra in palabras_negativas)\n",
        "\n",
        "    if positivas > negativas:\n",
        "        return \"positivo\"\n",
        "    elif negativas > positivas:\n",
        "        return \"negativo\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Integración de las funcionalidades avanzadas con el asistente\n",
        "def mejorar_asistente():\n",
        "    \"\"\"\n",
        "    Mejora el asistente con las nuevas funcionalidades\n",
        "    \"\"\"\n",
        "    # Clase base AsistenteVirtual simulada para el ejemplo\n",
        "    class AsistenteVirtual:\n",
        "        def __init__(self):\n",
        "            self.nombre = \"Asistente Virtual Básico\"\n",
        "\n",
        "    # Clase mejorada\n",
        "    class AsistenteVirtualMejorado(AsistenteVirtual):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.nombre = \"Asistente Virtual Mejorado\"\n",
        "            self.recursos_educativos = {\n",
        "                \"matematicas\": [\"Khan Academy\", \"Wolfram Alpha\"],\n",
        "                \"ciencias\": [\"NASA Education\", \"National Geographic Kids\"]\n",
        "            }\n",
        "\n",
        "        def analizar_sentimiento_usuario(self, texto):\n",
        "            return analisis_sentimiento(texto)\n",
        "\n",
        "        def recomendar_recursos(self, tema, sentimiento=None):\n",
        "            recursos = self.recursos_educativos.get(tema, [\"No tengo recursos para ese tema.\"])\n",
        "            if sentimiento == \"negativo\":\n",
        "                recursos.append(\"También recomiendo tomar descansos regulares.\")\n",
        "            return recursos\n",
        "\n",
        "        def crear_plan_estudio(self, tema, nivel):\n",
        "            return {\n",
        "                \"tema\": tema,\n",
        "                \"nivel\": nivel,\n",
        "                \"plan\": [\n",
        "                    f\"1. Revisar conceptos básicos de {tema}\",\n",
        "                    f\"2. Hacer ejercicios de nivel {nivel}\",\n",
        "                    \"3. Revisar errores\",\n",
        "                    \"4. Avanzar a temas más complejos\"\n",
        "                ]\n",
        "            }\n",
        "\n",
        "    return AsistenteVirtualMejorado\n",
        "\n",
        "# Casos de uso y ejemplos\n",
        "def mostrar_casos_uso():\n",
        "    \"\"\"\n",
        "    Muestra ejemplos de cómo utilizar el asistente en diferentes escenarios\n",
        "    \"\"\"\n",
        "    # Caso 1: Apoyo académico\n",
        "    print(\"=== Caso de uso: Apoyo académico ===\")\n",
        "    ejercicios = generador_ejercicios(\"matematicas\", \"medio\")\n",
        "    print(\"Ejercicios generados:\", ejercicios[:2])\n",
        "\n",
        "    AsistenteMejorado = mejorar_asistente()\n",
        "    asistente = AsistenteMejorado()\n",
        "    plan = asistente.crear_plan_estudio(\"matematicas\", \"intermedio\")\n",
        "    print(\"Plan de estudio:\", plan[\"plan\"][0])\n",
        "\n",
        "    # Caso 2: Apoyo emocional\n",
        "    print(\"\\n=== Caso de uso: Apoyo emocional ===\")\n",
        "    sentimiento = analisis_sentimiento(\"Hoy me siento muy feliz y contento\")\n",
        "    print(\"Análisis de sentimiento:\", sentimiento)\n",
        "\n",
        "    # Caso 3: Recomendaciones personalizadas\n",
        "    print(\"\\n=== Caso de uso: Recomendaciones personalizadas ===\")\n",
        "    recursos = asistente.recomendar_recursos(\"ciencias\", \"positivo\")\n",
        "    print(\"Recursos recomendados:\", recursos[:2])\n",
        "\n",
        "# Prueba el sistema completo con un ejemplo real\n",
        "def prueba_sistema_completo():\n",
        "    \"\"\"\n",
        "    Prueba el sistema completo con un ejemplo de conversación\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Prueba del sistema completo ===\")\n",
        "    AsistenteMejorado = mejorar_asistente()\n",
        "    asistente = AsistenteMejorado()\n",
        "\n",
        "    # Simular interacción\n",
        "    usuario = \"Estoy estudiando matemáticas pero me siento un poco abrumado\"\n",
        "\n",
        "    # Analizar sentimiento\n",
        "    sentimiento = asistente.analizar_sentimiento_usuario(usuario)\n",
        "    print(f\"Detecto que tu estado emocional es: {sentimiento}\")\n",
        "\n",
        "    # Recomendar recursos\n",
        "    if \"matemáticas\" in usuario.lower():\n",
        "        recursos = asistente.recomendar_recursos(\"matematicas\", sentimiento)\n",
        "        print(\"Te recomiendo estos recursos:\", recursos)\n",
        "\n",
        "    # Generar plan de estudio\n",
        "    plan = asistente.crear_plan_estudio(\"matematicas\", \"principiante\")\n",
        "    print(\"\\nAquí tienes un plan de estudio para empezar:\")\n",
        "    for paso in plan[\"plan\"]:\n",
        "        print(f\"- {paso}\")\n",
        "\n",
        "# Ejecutar demostración\n",
        "if __name__ == \"__main__\":\n",
        "    mostrar_casos_uso()\n",
        "    prueba_sistema_completo()"
      ],
      "metadata": {
        "id": "yY0TmBklD2jQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}